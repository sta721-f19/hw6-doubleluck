\documentclass{article}
\usepackage{url,hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{macros}
\usepackage{fullpage}
\def\xtx{\X^T\X}
\def\bh{\hat{\b}}
\def\bt{\tilde{\b}}

\begin{document}
\title{Homework 5: STA 721 Fall19}
\author{Your Names}
\date{\today}
\maketitle

You may work in groups of two for this assignment.  Both team members should make commits on github and contribute equally to the project. Please look at before class  in case there are questions or clarifications needed (or post on Piazza). This complete assignment should be completed using knitr/Sweave.

\begin{enumerate}
  \item  As a variation on the simulation study in Nott \& Kohn (Biometrika
2005), we will explore shrinkage estimators in the normal linear model
\begin{equation}
  \label{eq:model}
Y \sim N(X \beta, I_n\sigma^2)
\end{equation}
where $X$ has been generated to have a given correlation structure.

<<data, cache=T>>=   # change to echo=FALSE for submission please!
n = 50
sigma = 2.5
betatrue = c(4,2,0,0,0,-1,0,1.5, 0,0,0,1,0,.5,0,0,0,0,-1,1,4)
#          int|    X1                            | X2     |X3
# set the random seed so that we can replicate results. Update your seed to be your team number.

set.seed(42)

# number of simulated data sets
nsim = 100

fname = rep("df", nsim)

# create 100 datasets
for (i in 1:nsim) {
  Z = matrix(rnorm(n*10, 0, 1), ncol=10, nrow=n)
  X1 = cbind(Z, (Z[,1:5] %*% c(.3, .5, .7, .9, 1.1) %*% t(rep(1,5)) +
             matrix(rnorm(n*5, 0, 1), ncol=5, nrow=n)))
  X2 <- matrix(rnorm(n*4,0,1), ncol=4, nrow=n)
  X3 <- X2[,4]+rnorm(n,0,sd=0.1)
  X <- cbind(X1,X2,X3)
  X = sweep(X, 2, apply(X,2, mean))  # subtract off the column means
  Y = betatrue[1] + X %*% betatrue[-1] + rnorm(n,0,sigma)  # does not have a column of ones for the intercept
  df = data.frame(Y, X)
  fname[i] = paste("df", as.character(i), sep="")
  save(df, file=fname[i])
}

@


Two of the
variables have a correlation of near 0.99, with the others more
modest.  Of the 20 variables, only 8 are related to $Y$.

\begin{enumerate}
  \item  Calculate the $\E[(\hat{\beta} - \beta)^T(\hat{\beta} - \beta)]$, the expected MSE for OLS under the full model.  (should be review!)
  
  For $\sigma^2$ known, $\bh|\sigma^2 \sim N(\b, \sigma^2(\xtx)^{-1})$, for $\sigma^2$ unknown, . 
  

  
  
  \item   For each simulation, the OLS coefficients are found and an observed $\MSE = (\hat{\beta}^{(s)} - \beta)^T(\hat{\beta}^{(s)} -  \beta)$ is computed for each of the $s$ simulated datasets.
<<OLS, cache=TRUE>>= # hide code for submission
MSE.OLS  = rep(NA,nsim)

for( i in 1:nsim) {
  rm(df)
  load(fname[i])
  nk.ols = lm(Y ~ ., data=df)
  coef.ols = coef(nk.ols)
  MSE.OLS[i] = sum((betatrue - coef.ols)^2)
#  print(c(i, MSE.OLS[i]))  for the truly bored student
}
@

Does  the average of observed MSEs, \Sexpr{round(mean(MSE.OLS), 2)}, provide a good estimate  of the average of $E[(\hat{\beta} - \beta)^T(\hat{\beta} - \beta)]$? What does the distribution of observed MSE minus expected MSEs look like?   Do you think you should increase the number of simulated data sets?


<<>>=

expected_mse <- numeric(nsim) 

for (i in 1:nsim) {
  rm(df)
  load(fname[i])
  nk.ols = lm(Y~., data=df)
  X <- as.matrix(df[,-1])
  xtx <- t(X)%*%X
  ssr <- sum(nk.ols$residuals^2)
  deg_freedom = n-ncol(X)
  
  # TODO should this be norm? 
  expected_mse[i] <- sum((deg_freedom)/(deg_freedom-2) * sqrt(ssr/diag(solve(xtx)))/deg_freedom)  # variance of t times standard error for beta hat
}

diff_mse = MSE.OLS - expected_mse
hist(diff_mse, breaks=20)
@


\item For each of the data sets run lasso, ridge regression (lm.ridge from MASS or other), and the horseshoe (bhs from monomvn package on CRAN or other) to provide estimates of $\beta$  MSE, variance and squared bias for each simulated dataset; be careful about which methods standardize variables.

<<ridge>>=
library(MASS)


betahat.ridge <- matrix(nrow=nsim, ncol=length(betatrue))


for( i in 1:nsim) {
  rm(df)
  load(fname[i])
  nk.ridge = lm.ridge(Y ~ ., data=df, lambda=1) # how are we supposed to set lambda here? 
  betahat.ridge[i,] = coef(nk.ridge) # store coefs
  
  # MSE.RIDGE[i] = sum((betatrue - coef.ridge)^2)
  
#  print(c(i, MSE.OLS[i]))  for the truly bored student
}

squared_mse <- apply( (betahat.ridge - matrix(rep(betatrue, nsim), ncol=length(betatrue), byrow = T))^2, 1, sum) # bias for each sim
variance <- apply(betahat.ridge, 2, var) # estimate of variance
mse.ridge <- variance + squared_bias
@

<<lasso>>=

@

<<horseshoe>>=

@

\item Derive the full conditional distributions for the Generalized double Pareto model of Armagan, Dunson, and Lee (2011)\footnote{Artin Armagan, David Dunson, \& Jaeyong Lee (2011). Generalized double Pareto shrinkage Statistica Sinica 23 (2013), 119-143 arXiv: 1104.0861v4 } (\url{https://arxiv.org/pdf/1104.0861v4.pdf})
(see Section 3) but with the Independent Jeffreys prior on $\beta_0$ and $\phi \equiv 1/\sigma^2$.


We assume here a centered, rotated $\X$. Also, $\tau, \lambda$ represent diagonal matrices in our notation, with $\tau_{j}$ denoting a particular diagonal element. 

We can write the joint distribution $p(\Y, \b, \tau, \lambda, \phi) \propto \phi^{n/2}exp(-frac{\phi}{2}(\b-\bh)^T(\xtx)(\b-\bh))exp(-frac{\phi}{2}SSE)exp(-\frac{\phi}{2}(\b_0-\bar{y})^2n)p(\b|\tau,\phi)p(\b_0, \phi)p(\tau|\lambda)p(\lambda)$

So $\begin{aligned} \b \propto exp(-\frac{\phi}{2}\b^T\tau^{-1}\b)exp(-frac{\phi}{2}(\b-\bh)^T(\xtx)(\b-\bh))\\
\propto exp(-\frac{\phi}{2}(\b^T(\xtx+\tau^{-1})\b -2\b^T(\xtx+\tau^{-1})(\xtx+\tau^{-1})^{-1}\xtx\bh ))
\end{aligned}$ 

We recognize a normal kernel, thus $\b \sim N((\xtx+\tau^{-1})^{-1}\xtx\bh, (\xtx+\tau^{-1})^{-1})$. We can simplify with the identity $\xtx\bh = \X^T\Y$ to obtain the formula in Armagan, Dunson and Lee of $\b \sim N((\xtx+\tau^{-1})^{-1}\X^T\Y, (\xtx+\tau^{-1})^{-1}/\phi)$

It is simpler to find $\tau_{j}$ components individually 
$$\begin{aligned} \tau_j \propto p(\b|\tau_j,\phi)p(\tau_j|\lambda_j) \propto  (\phi/\tau_j)^{1/2}exp(-\frac{\phi}{2}\b_j^T\b_j/\tau_j)exp(-\lambda^2\tau_j)\\
\propto \tau_j^{-1/2}exp(-\frac{1}{2}(\phi\b_j^2/\tau_j+\lambda_j^2\tau_j))\\
\end{aligned}$$
We can recognize a Generalized Inverse Gaussian kernel, with parameters $\mu = 1/2, \nu=\lambda_j^2, \xi=\b_j^2/\tau$\\

$$\begin{aligned} \phi \propto \phi^{n/2}exp(-frac{\phi}{2}(\b-\bh)^T(\xtx)(\b-\bh))exp(-frac{\phi}{2}SSE)exp(-\frac{\phi}{2}(\b_0-\bar{y})^2n)p(\b|\tau,\phi)p(\b_0, \phi)\\ 
\propto \phi^{n/2}exp(-frac{\phi}{2}((\b-\bh)^T(\xtx)(\b-\bh)+SSE + n(b_0-\bar{y})^2 + \b^T\tau^{-1}\b)) |\tau/\phi|^{-1/2} 1/\phi \\ 
\propto \phi^{(n+p)/2-1}exp(-frac{\phi}{2}((\b-\bh)^T(\xtx)(\b-\bh)+SSE + n(b_0-\bar{y})^2 + \b^T\tau^{-1}\b))
\end{aligned}$$

We can recognize a (rather ugly) Gamma kernel, with parameters $\alpha = (n+p)/2, \beta=((\b-\bh)^T(\xtx)(\b-\bh)+SSE + n(b_0-\bar{y})^2 + \b^T\tau^{-1}\b)/2$\\

Since $\b_0 \propto exp(-\frac{\phi}{2}(n(b_0-\bar{y})^2))$, we can recognize a normal kernel, with $\mu=\bar{y}, \sigma^2 = 1/(n\phi)$\\


Finally, $\lambda_j \propto p(\lambda_j|\tau_j)p(\lambda_j) \propto \lambda_j^2 exp(-\frac{1}{2}\lambda_j^2/2)\lambda_j^{\alpha-1}exp(-\lambda_j\eta) = \lambda^{(\alpha+2)-1}exp(-\frac{1}{2}(\lambda_j^2\tau_j+2\lambda_j\eta)) \propto \lambda^{(\alpha+2)-1}exp(-\frac{\tau_j}{2}(\lambda_j+\frac{\eta}{\tau_j})^2)$. We can recognize a Generalized gamma distribution \url{https://en.wikipedia.org/wiki/Generalized_gamma_distribution} with parameters $a=\sqrt{\frac{2}{\tau_j}}, p=2, d=\alpha+2$, so $\lambda_j+\frac{\eta}{\tau_j} \sim GenGamma(\frac{2}{\tau_j}, 2, \alpha+2)$\\

Also note that if $\alpha+2 \in \mathbb{N}$, then we can utilize a scaled $\chi$ distribution with $\alpha+2$ degrees of freedom. 

It is alternatively possible to return to our joint distribution, and marginalize $\tau$ from the distribution entirely.\\
Since $\tau_j \sim GIG(\mu=1/2, \nu=\lambda_j^2, \xi=\phi\b_j^2)$, our integral will be proportional to the inverse of the normalizing constant$ K_{\mu}(\sqrt{\nu\xi})(\nu/\xi)^{\mu/2}$, where $K_{1/2}$ is a modified Bessel function of the second kind. Resources online here \url{http://functions.wolfram.com/Bessel-TypeFunctions/BesselK/introductions/Bessels/05/} indicate that $K_{1/2}(z) = \frac{e^{-z}}{\sqrt{z}}$, so we obtain a normalizing constant overall of $e^{-\sqrt{\nu\xi}} (\nu\xi)^{1/4}(\nu)^{1/4}\xi^{-1/4} = e^{-\sqrt{\nu\xi}}(\nu)^{1/2} = e^{-\lambda_j|\b_j|\sqrt{\phi}}\lambda_j$

Returning to our original joint distribution and dividing by this normalizing constant, we can obtain $\lambda_j \propto exp(-|\b_j|\lambda_j^{-1}\sqrt{\phi})\lambda_j^{-1}\lambda_j^2\lambda^{\alpha-1}exp(-\lambda_j\eta) = \lambda_j^{(\alpha+1)-1}exp(-\lambda(|\b_j|\sqrt{\phi}+\eta))$, and obtain a Gamma kernel with $\alpha=\alpha+1, \beta=|\b_j|\sqrt{\phi}+\eta$


\item  Emplement the Generalized Double Pareto  and compare to the above methods, using the default $\alpha = \eta = 1$.
See
    Michael Lindon's implementation in R using Rcpp \url{https://michaellindon.github.io/lindonslog/mathematics/statistics/generalized-double-pareto-shrinkage-priors-sparse-estimation-regression/index.html}  or implement in JAGS.  Be careful about standardizing the predictors and change of variables for coefficients when computing the MSE!

\item In terms of MSE, which method(s) appears to be best (look  at average MSE and side-by-side boxplots)?  Which method has the least bias?  Which method has the most variability?

\item  (Optional)- repeat the above simulations, but now consider predictive MSE for  predicting new $\Y^*$'s at new $\X^*$ values with the same correlation  structure.  Are the methods that are best for estimating $\b$ also  best for estimating $\Y^*$?   Which seems to be harder?

\end{enumerate}

\item  For  the Gaussian ridge prior,  the lasso (Double exponential), independent Cauchy,  and the Generalized Double pareto, show whether the prior satisfy's any/all of the 3 conditions in Fan and Li (2011) \url{https://www.tandfonline.com/doi/abs/10.1198/016214501753382273}.

Lasso:
For $\b_j \sim DE(0, \lambda)$, we obtain $p_\lambda(|\b_j|) = ln(\lambda) + \frac{|\b_j|}{\lambda}$, and thus $f(|\b_j|) = |\b_j| + \frac{\delta p_\lambda(|\b_j|)}{\delta |\b_j|} = |\b_j| + \frac{1}{\lambda}$\\

We note that $\b_j = 0 \implies f(|\b_j|) = \frac{1}{\lambda}$, thus the Lasso qualifies as a thresholding rule.\\
Also, since $f(|\b_j|)$is clearly minimized at $|\b_j| = 0$, we can see that the Lasso provides a continuous estimator.\\
However, since $\lim_{\b_j \to \infty}\frac{\delta p_\lambda(|\b_j|)}{\delta |\b_j|} = \frac{1}{\lambda} \ne 0$, the estimator cannot be considered approximately unbiased. 



Independent Cauchy:\\
For $\b_j \sim Cauchy(0, \lambda)$, we obtain $p_\lambda(|\b_j|) = -ln(\lambda) + ln(|\b_j|^2+\lambda^2)$\\
Then $f(|\b_j|) = |\b_j| + \frac{2|\b_j|}{\lambda^2+|\b_j|^2}$. $|\b_j|=0 \implies f(|\b_j|) = 0$, so this does not qualify as a thresholding rule. It is also easy to see that $f(|\b_j|)$ is monotonically increasing as $|\b_j|$ increases, so the minimum achieved at $\b_j=0$ is singular, thus our estimator will be continuous. Also, since $\lim_{\b_j \to \infty}\frac{\delta p_\lambda(|\b_j|)}{\delta |\b_j|} = 0$, this is an approximately unbiased estimator for large $|\b_j|$. 

Generalized Double Pareto:
For $\b_j \sim GDP(\eta/\lambda, \lambda)$, we obtain $p_\lambda(|\b_j|) = ln(\eta/\lambda) + (\lambda+1)ln(1 + \frac{|\b_j|}{\eta})$, thus $f(|\b_j|) = |\b_j| + \frac{\lambda+1}{1+\frac{|\b_j|}{\eta}}\frac{1}{\eta}$ \\
Setting $\b_j = 0 \implies f(|\b_j|) = \frac{\lambda+1}{\eta}$, so for positive $\lambda, \eta$, we obtain a thresholding rule.\\
We can also see that $\lim_{\b_j \to \infty}\frac{\delta p_\lambda(|\b_j|)}{\delta |\b_j|} = \lim_{\b_j \to \infty}\frac{\lambda+1}{1+\frac{|\b_j|}{\eta}}\frac{1}{\eta} = 0$, so this represents an approximately unbiased estimator.\\
Finally, we evaluate whether $f(|\b_j|)$ attains a minimum at $0$ in order to determine the continuity of the estimator. We set $\frac{\delta f(|\b_j|)}{\delta |\b_j|} = 1 - \frac{\lambda+1}{\eta^2}(\frac{1}{1+\frac{|\b_j|}{\eta}})^2 = 0$ to identify extrema. Rearranging and expanding a bit, we obtain $|\b_j| + 2|\b_j|\eta + \eta^2 - (\lambda+1)=0$\\
Applying the quadratic formula, we obtain solutions as $|\b_j| = -\eta \pm \sqrt{\lambda+1}$. 

Since $|\b_j| \in \mathbb{R}^{+}$, a solution for $-\eta + \sqrt{\lambda+1}$  will exist if and only if $\eta\le\sqrt{\lambda+1}$. Some tedious algebra will show that these points, if they exist, are guaranteed to be less than $f(0)$, so we must require $\eta \ge \sqrt{\lambda+1}$. The original Armagan, Dunson, and Lee paper sets $\eta = \sqrt{\lambda+1}$, which is sufficient to guarantee a continuous estimator for $|\b_j|$. 



\end{enumerate}
\end{document}
