---
title: "STA721 HW6"
author: "Zheng Yuan        Evan Wyse"
date: "2019/10/30"
output: 
  pdf_document:
    includes:
      in_header: macros.tex
header_includes:
  macros.tex
---

\def\xtx{\X^T\X}
\def\bh{\hat{\b}}
\def\bt{\tilde{\b}}
\def\X{\mathbf{X}}
\def\Y{\mathbf{Y}}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

```{r, cache=T, echo=FALSE}
n = 50
sigma = 2.5
betatrue = c(4,2,0,0,0,-1,0,1.5, 0,0,0,1,0,.5,0,0,0,0,-1,1,4)
#          int|    X1                            | X2     |X3
# set the random seed so that we can replicate results. Update your seed to be your team number.

set.seed(42)

# number of simulated data sets
nsim = 100

fname = rep("df", nsim)

# create 100 datasets
for (i in 1:nsim) {
  Z = matrix(rnorm(n*10, 0, 1), ncol=10, nrow=n)
  X1 = cbind(Z, (Z[,1:5] %*% c(.3, .5, .7, .9, 1.1) %*% t(rep(1,5)) +
             matrix(rnorm(n*5, 0, 1), ncol=5, nrow=n)))
  X2 <- matrix(rnorm(n*4,0,1), ncol=4, nrow=n)
  X3 <- X2[,4]+rnorm(n,0,sd=0.1)
  X <- cbind(X1,X2,X3)
  X = sweep(X, 2, apply(X,2, mean))  # subtract off the column means
  Y = betatrue[1] + X %*% betatrue[-1] + rnorm(n,0,sigma)  # does not have a column of ones for the intercept
  df = data.frame(Y, X)
  fname[i] = paste("df", as.character(i), sep="")
  save(df, file=fname[i])
}
```

### (a)

First,the mle for $\hat \beta$ conditional on X is $$\hat \beta=(X^TX)^{-1}X^TY$$\newline

The expected MSE for OLS under the full model can be written as 

$$E[(\beta-\hat \beta)^T (\beta- \hat \beta)]=tr(IVar(\hat \beta))+(E(\beta-\hat \beta))^TE(\beta-\hat \beta)=tr(Var(\hat \beta))+(E(\beta-\hat \beta))^TE(\beta-\hat \beta),$$ and
$$E(\beta-\hat \beta)=\beta-E(\hat\beta)=\beta-(X^TX)^{-1}X^TX\beta=\beta-\beta=0$$
$$Var(\hat\beta)=Var((X^TX)^{-1}X^TY)=\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1}=\sigma^2(X^TX)^{-1}$$
so $$E[(\beta-\hat \beta)^T (\beta- \hat \beta)]=tr(\sigma^2(X^TX)^{-1})=\sigma^2\sum_{i=1}^{p}\frac{1}{\lambda_i},$$

here $\lambda_i's$ are the eigenvalues of $X^TX$.\newline

### (b)

First, we calculate the average of observed MSEs,

```{r,cache=TRUE, echo=FALSE}
MSE.OLS  = rep(NA,nsim)

for( i in 1:nsim) {
  rm(df)
  load(fname[i])
  nk.ols = lm(Y ~ ., data=df)
  coef.ols = coef(nk.ols)
  MSE.OLS[i] = sum((betatrue - coef.ols)^2)
#  print(c(i, MSE.OLS[i]))  for the truly bored student
}

mean(MSE.OLS)
```
In order to see whether the average of observed MSEs provides a good estimate of the expected MSE, we can also calculate the expected MSE analytically, that is 
$$E[(\beta-\hat \beta)^T (\beta- \hat \beta)]=tr(\sigma^2(X^TX)^{-1})$$
and compare with it.

```{r}
EMSE=(sigma^2)*sum(diag(solve(t(X)%*%X)))
EMSE
```

After comparison, there is still a unignorable difference between the average of observed MSEs and the expected MSE, so it does not provide a good estimate actually.\newline

The distribution of observed MSE minus expected MSEs:

```{r}
hist(MSE.OLS-EMSE, freq = F, breaks=20);lines(density(MSE.OLS-EMSE))
```
\newline As we can see, the distribution of observed MSE minus expected MSEs looks like a normal distribution centering around its mean, which is around -35. In this case, I don't think increasing the number of simulated date sets would help, because the distribution of observed MSE minus expected MSEs does not center around 0, by the Central Limit Theorem, the average of observed MSEs must not converge to expected MSE in the end. Therefore, in fact, the more simulated date sets we have, the more difference between the average and the expected value there will be, until the difference is stable around a value.\newline

### (c)
```{r,cache=TRUE}
library(lars)
library(MASS)
library(matrixStats)
library(monomvn)

OLS.MSE = rep(0,100) 
lasso.MSE = rep(0,100) 
ridge.MSE = rep(0,100) 
bhs.MSE = rep(0,100)

Coef.OLS = matrix(rep(0,2100),nrow=100) ## record OLS estimates of beta for each simulation 
Coef.lasso = matrix(rep(0,2000),nrow=100)## record lasso estimates of beta for each simulation 
Coef.ridge = matrix(rep(0,2100),nrow=100)## record ridge estimates of beta for each simulation 
Coef.bhs = matrix(rep(0,2100),nrow=100)## record bhs estimates of beta for each simulation 


for( i in 1:100) {
  rm(df)
  load(fname[i])
  X = as.matrix(df[,-1]); Y = df[,1]
  X.scale = scale(X); n = length(Y); X.mean = colMeans(X); X.sd = colSds(X)
  coeftrue = betatrue
  coeftrue[1] = betatrue[1]+sum(betatrue[-1]*X.mean) 
  coeftrue[-1] = betatrue[-1]*X.sd
  
  
  ## ols
  nk.ols = lm(Y ~ X)
  coef.ols = coef(nk.ols)
  OLS.MSE[i] = sum((betatrue - coef.ols)^2)
  Coef.OLS[i,1:21]=coef.ols
  
  ## lasso
  nk.lasso = lars(X, Y, type="lasso")
  coef.lasso = coef(nk.lasso, s=which.min(nk.lasso$Cp))
  lasso.MSE[i] = sum((betatrue[-1] - coef.lasso)^2)
  Coef.lasso[i,1:20]=coef.lasso
  
  ## ridge
  seq.lambda = seq(0,10,0.01)
  nk.ridge = lm.ridge(Y ~ X, lambda = seq.lambda)
  lambda = seq.lambda[which.min(nk.ridge$GCV)]
  nk.ridge = lm.ridge(Y ~ X, lambda = lambda)
  coef.ridge = coef(nk.ridge)
  ridge.MSE[i] = sum((betatrue - coef.ridge)^2)
  Coef.ridge[i,1:21]=coef.ridge
  
  ## horseshoe
  nk.bhs = blasso(X, Y, case="hs", RJ=FALSE, normalize=F, verb=0)
  coef.bhs = c(mean(nk.bhs$mu), apply(nk.bhs$beta, 2, mean))
  bhs.MSE[i] = sum((betatrue - coef.bhs)^2)
  Coef.bhs[i,1:21]=coef.bhs
  
}
## Observed MSE for each simulation
boxplot(OLS.MSE, lasso.MSE, ridge.MSE, bhs.MSE, names=c("OLS", "lasso", "ridge", "horseshoe"), main="Observed MSE")

## eatimation of squared bias and variance for each estimation

mean.OLS=apply(Coef.OLS,2, mean)
bias2.OLS=(mean.OLS-betatrue)^2
var.OLS = apply(Coef.OLS, 2, var)
plot(bias2.OLS, xlab="index of beta",ylab="OLS Squared Bias" )
plot(var.OLS, xlab ="index of beta", ylab="OLS Variance" )

mean.lasso=apply(Coef.lasso,2, mean)
bias2.lasso=(mean.lasso-betatrue[-1])^2
var.lasso = apply(Coef.lasso, 2, var)
plot(bias2.lasso, xlab="index of beta",ylab="LASSO Squared Bias" )
plot(var.lasso, xlab ="index of beta", ylab="LASSO Variance" )

mean.ridge=apply(Coef.ridge,2, mean)
bias2.ridge=(mean.ridge-betatrue)^2
var.ridge = apply(Coef.ridge, 2, var)
plot(bias2.ridge, xlab="index of beta",ylab="Ridge Squared Bias" )
plot(var.ridge, xlab ="index of beta", ylab="Ridge Variance" )

mean.bhs=apply(Coef.bhs,2, mean)
bias2.bhs=(mean.bhs-betatrue)^2
var.bhs = apply(Coef.bhs, 2, var)
plot(bias2.bhs, xlab="index of beta",ylab="Horseshoe Prior Squared Bias" )
plot(var.bhs, xlab ="index of beta", ylab="Horseshoe Prior Variance" )
```
\newline

### (d)
We assume here a centered, rotated $\X$. Also, $\tau, \lambda$ represent diagonal matrices in our notation, with $\tau_{j}$ denoting a particular diagonal element. 

We can write the joint distribution $p(\Y, \b, \tau, \lambda, \phi) \propto \phi^{n/2}exp(-\frac{\phi}{2}(\b-\bh)^T(\xtx)(\b-\bh))exp(-\frac{\phi}{2}SSE)exp(-\frac{\phi}{2}(\b_0-\bar{y})^2n)p(\b|\tau,\phi)p(\b_0, \phi)p(\tau|\lambda)p(\lambda)$

So $$\begin{aligned} \b \propto exp(-\frac{\phi}{2}\b^T\tau^{-1}\b)exp(-frac{\phi}{2}(\b-\bh)^T(\xtx)(\b-\bh))\\
\propto exp(-\frac{\phi}{2}(\b^T(\xtx+\tau^{-1})\b -2\b^T(\xtx+\tau^{-1})(\xtx+\tau^{-1})^{-1}\xtx\bh ))
\end{aligned}$$

We recognize a normal kernel, thus $\b \sim N((\xtx+\tau^{-1})^{-1}\xtx\bh, (\xtx+\tau^{-1})^{-1})$. We can simplify with the identity $\xtx\bh = \X^T\Y$ to obtain the formula in Armagan, Dunson and Lee of $\b \sim N((\xtx+\tau^{-1})^{-1}\X^T\Y, (\xtx+\tau^{-1})^{-1}/\phi)$

It is simpler to find $\tau_{j}$ components individually 
$$\begin{aligned} \tau_j \propto p(\b|\tau_j,\phi)p(\tau_j|\lambda_j) \propto  (\phi/\tau_j)^{1/2}exp(-\frac{\phi}{2}\b_j^T\b_j/\tau_j)exp(-\lambda^2\tau_j)\\
\propto \tau_j^{-1/2}exp(-\frac{1}{2}(\phi\b_j^2/\tau_j+\lambda_j^2\tau_j))\\
\end{aligned}$$
We can recognize a Generalized Inverse Gaussian kernel, with parameters $\mu = 1/2, \nu=\lambda_j^2, \xi=\b_j^2/\tau$\\

$$\begin{aligned} \phi \propto \phi^{n/2}exp(-\frac{\phi}{2}(\b-\bh)^T(\xtx)(\b-\bh))exp(-\frac{\phi}{2}SSE)exp(-\frac{\phi}{2}(\b_0-\bar{y})^2n)p(\b|\tau,\phi)p(\b_0, \phi)\\ 
\propto \phi^{n/2}exp(-\frac{\phi}{2}((\b-\bh)^T(\xtx)(\b-\bh)+SSE + n(b_0-\bar{y})^2 + \b^T\tau^{-1}\b)) |\tau/\phi|^{-1/2} 1/\phi \\ 
\propto \phi^{(n+p)/2-1}exp(-\frac{\phi}{2}((\b-\bh)^T(\xtx)(\b-\bh)+SSE + n(b_0-\bar{y})^2 + \b^T\tau^{-1}\b))
\end{aligned}$$

We can recognize a (rather ugly) Gamma kernel, with parameters $\alpha = (n+p)/2, \beta=((\b-\bh)^T(\xtx)(\b-\bh)+SSE + n(b_0-\bar{y})^2 + \b^T\tau^{-1}\b)/2$\\

Since $\b_0 \propto exp(-\frac{\phi}{2}(n(b_0-\bar{y})^2))$, we can recognize a normal kernel, with $\mu=\bar{y}, \sigma^2 = 1/(n\phi)$\\


Finally, $\lambda_j \propto p(\lambda_j|\tau_j)p(\lambda_j) \propto \lambda_j^2 exp(-\frac{1}{2}\lambda_j^2/2)\lambda_j^{\alpha-1}exp(-\lambda_j\eta) = \lambda^{(\alpha+2)-1}exp(-\frac{1}{2}(\lambda_j^2\tau_j+2\lambda_j\eta)) \propto \lambda^{(\alpha+2)-1}exp(-\frac{\tau_j}{2}(\lambda_j+\frac{\eta}{\tau_j})^2)$. We can recognize a Generalized gamma distribution \url{https://en.wikipedia.org/wiki/Generalized_gamma_distribution} with parameters $a=\sqrt{\frac{2}{\tau_j}}, p=2, d=\alpha+2$, so $\lambda_j+\frac{\eta}{\tau_j} \sim GenGamma(\frac{2}{\tau_j}, 2, \alpha+2)$\\

Also note that if $\alpha+2 \in \mathbb{N}$, then we can utilize a scaled $\chi$ distribution with $\alpha+2$ degrees of freedom. 

It is alternatively possible to return to our joint distribution, and marginalize $\tau$ from the distribution entirely.\\
Since $\tau_j \sim GIG(\mu=1/2, \nu=\lambda_j^2, \xi=\phi\b_j^2)$, our integral will be proportional to the inverse of the normalizing constant $K_{\mu}(\sqrt{\nu\xi})(\nu/\xi)^{\mu/2}$, where $K_{1/2}$ is a modified Bessel function of the second kind. Resources online here \url{http://functions.wolfram.com/Bessel-TypeFunctions/BesselK/introductions/Bessels/05/} indicate that $K_{1/2}(z) = \frac{e^{-z}}{\sqrt{z}}$, so we obtain a normalizing constant overall of $e^{-\sqrt{\nu\xi}} (\nu\xi)^{1/4}(\nu)^{1/4}\xi^{-1/4} = e^{-\sqrt{\nu\xi}}(\nu)^{1/2} = e^{-\lambda_j|\b_j|\sqrt{\phi}}\lambda_j$

Returning to our original joint distribution and dividing by this normalizing constant, we can obtain $\lambda_j \propto exp(-|\b_j|\lambda_j^{-1}\sqrt{\phi})\lambda_j^{-1}\lambda_j^2\lambda^{\alpha-1}exp(-\lambda_j\eta) = \lambda_j^{(\alpha+1)-1}exp(-\lambda(|\b_j|\sqrt{\phi}+\eta))$, and obtain a Gamma kernel with $\alpha=\alpha+1, \beta=|\b_j|\sqrt{\phi}+\eta$


### (e)

```{r, cache=T}
library(lars)
library(MASS)
library(matrixStats)
library(monomvn)
library(Rcpp)
library(RcppArmadillo)
sourceCpp("GDP.cpp")

Coef.gdp = matrix(rep(0,2000),nrow=100) ## record GDP estimates of beta for each simulation 
gdp.MSE = rep(0,100) ##MSE

for( i in 1:100){
  rm(df)
  load(fname[i])
  X = as.matrix(df[-1]); Y = df[,1]
  X.scale = scale(X); n = length(Y); X.mean = colMeans(X); X.sd = colSds(X)
  coeftrue = betatrue
  coeftrue[1] = betatrue[1]+sum(betatrue[-1]*X.mean) 
  coeftrue[-1] = betatrue[-1]*X.sd
  #Run GDP
  gdp=gdp_em(Y,X,1,1)
  Coef.gdp[i,1:20]=t(gdp$B)
  gdp.MSE[i] = sum((betatrue[-1] - t(gdp$B))^2)
  }

mean.gdp=apply(Coef.gdp,2, mean)
bias2.gdp=(mean.gdp-betatrue[-1])^2
var.gdp = apply(Coef.gdp, 2, var)

plot(bias2.gdp, xlab="index of beta",ylab="GDP Squared Bias" )
plot(var.gdp, xlab ="index of beta", ylab="GDP Variance" )

## Observed MSE for each simulation
boxplot(gdp.MSE, names=c("GDP"), main="Observed MSE")
```

### (f)

```{r}
boxplot(OLS.MSE, lasso.MSE, ridge.MSE, bhs.MSE, gdp.MSE, names=c("OLS", "lasso", "ridge", "horseshoe","GDP"), main="Observed MSE")
```
\newline
From the result, we can see that estimation from Horseshoe Prior enjoys the lowest MSE. Blasso appears to be the best method. In terms of bias and variability, OLS estimator has both the least squared bias and the highest variance, besides, the variance seems far higher than those of other methods. Therefore, the other methods all performs better than OLS estimator in overall sense. It means that for these datas sets, shrinkage methods works much better than the normal OLS, if we can sacrifies a little bit bias, we can obatin much lower variance. When the covariates X has large correlation structure between each other, it's better to apple shrinkage methods than the traditional OLS.\newline

## Problem 2

### Ridge prior

$p_\lambda(|\theta|)=\lambda|\theta|^2$

$p_\lambda^\prime(|\theta|)=2\lambda|\theta|$

1. Unbiasedness: No.  $p_\lambda^\prime(|\theta|)=2\lambda|\theta|$ is not 0 when $|\theta|$ is large. So ridge is not unbiased.

2. Sparsity: No. $\min\ |\theta|+p_\lambda^\prime(|\theta|)=0$. So it does not have sparsity.

3. Continuity: Yes. $\min\ |\theta|+p_\lambda^\prime(|\theta|)$ is attained at 0. So it have continuity.

### Lasso prior

$p_\lambda(|\theta|)=\lambda|\theta|$

$p_\lambda^\prime(|\theta|)=2\lambda$

1. Unbiasedness: No.  $p_\lambda^\prime(|\theta|)=2\lambda$ is not 0  when $|\theta|$ is large. So ridge is not unbiased.

2. Sparsity: Yes. $\min\ |\theta|+p_\lambda^\prime(|\theta|)=|\theta|+2\lambda>0$. So it has sparsity.

3. Continuity: Yes. the minimum is attained at 0. So it have continuity.

### Cauchy prior

1. Unbiasedness:  Yes.

2. Sparsity: Yes, handling Sparsity.

3. Continuity: No, does not hold.

The horseshoe penalty function $p_\lambda(\theta_i)=-\log\log(1+\frac{2\tau^2}{\theta_i^2})$.

$p_\lambda^\prime(\theta_i)=\frac{4\tau^2/|\theta_i|^3}{(1+2\tau^2/|\theta_i|^2)\log(1+2\tau^2/|\theta_i|^2)}$. Then unbiasedness follows.
 
For $\theta\neq0$, $|\theta_i|+p_\lambda^\prime(\theta_i)=|\theta_i|+\frac{4\tau^2/|\theta_i|^3}{(1+2\tau^2/|\theta_i|^2)\log(1+2\tau^2/|\theta_i|^2)}$ is strictly larger than 0. So it hanles Sparsity.

Since $|\theta_i|+p_\lambda^\prime(\theta_i)\rightarrow \infty$, as $|\theta|\rightarrow 0$. So continuity does not hold.

### Generalized Double Pareto prior

The penalty function is $p_\lambda(|\theta|)=(\alpha+1)\log (|\theta|+\sigma\eta)$.

$p_\lambda^\prime(|\theta|)=(\alpha+1)\frac{1}{|\theta|}$ goes to 0 as $|\theta|\rightarrow \infty$. Then unbiasedness holds.

1. Unbiasedness:  Yes.

2. Sparsity: Holds when $\eta<2\sqrt{1+\alpha}$

3. Continuity: Yes when $\eta<=\sqrt{1+\alpha}$



