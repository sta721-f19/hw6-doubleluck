\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{url,hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{macros}
\usepackage{fullpage}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\title{Homework 5: STA 721 Fall19}
\author{Your Names}
\date{\today}
\maketitle

You may work in groups of two for this assignment.  Both team members should make commits on github and contribute equally to the project. Please look at before class  in case there are questions or clarifications needed (or post on Piazza). This complete assignment should be completed using knitr/Sweave.

\begin{enumerate}
  \item  As a variation on the simulation study in Nott \& Kohn (Biometrika
2005), we will explore shrinkage estimators in the normal linear model
\begin{equation}
  \label{eq:model}
Y \sim N(X \beta, I_n\sigma^2)
\end{equation}
where $X$ has been generated to have a given correlation structure.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n} \hlkwb{=} \hlnum{50}
\hlstd{sigma} \hlkwb{=} \hlnum{2.5}
\hlstd{betatrue} \hlkwb{=} \hlkwd{c}\hlstd{(}\hlnum{4}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlopt{-}\hlnum{1}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{1.5}\hlstd{,} \hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{.5}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlopt{-}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{4}\hlstd{)}
\hlcom{#          int|    X1                            | X2     |X3}
\hlcom{# set the random seed so that we can replicate results. Update your seed to be your team number.}

\hlkwd{set.seed}\hlstd{(}\hlnum{42}\hlstd{)}

\hlcom{# number of simulated data sets}
\hlstd{nsim} \hlkwb{=} \hlnum{100}

\hlstd{fname} \hlkwb{=} \hlkwd{rep}\hlstd{(}\hlstr{"df"}\hlstd{, nsim)}

\hlcom{# create 100 datasets}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{nsim) \{}
  \hlstd{Z} \hlkwb{=} \hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(n}\hlopt{*}\hlnum{10}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{),} \hlkwc{ncol}\hlstd{=}\hlnum{10}\hlstd{,} \hlkwc{nrow}\hlstd{=n)}
  \hlstd{X1} \hlkwb{=} \hlkwd{cbind}\hlstd{(Z, (Z[,}\hlnum{1}\hlopt{:}\hlnum{5}\hlstd{]} \hlopt{%*%} \hlkwd{c}\hlstd{(}\hlnum{.3}\hlstd{,} \hlnum{.5}\hlstd{,} \hlnum{.7}\hlstd{,} \hlnum{.9}\hlstd{,} \hlnum{1.1}\hlstd{)} \hlopt{%*%} \hlkwd{t}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{5}\hlstd{))} \hlopt{+}
             \hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(n}\hlopt{*}\hlnum{5}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{),} \hlkwc{ncol}\hlstd{=}\hlnum{5}\hlstd{,} \hlkwc{nrow}\hlstd{=n)))}
  \hlstd{X2} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(n}\hlopt{*}\hlnum{4}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{),} \hlkwc{ncol}\hlstd{=}\hlnum{4}\hlstd{,} \hlkwc{nrow}\hlstd{=n)}
  \hlstd{X3} \hlkwb{<-} \hlstd{X2[,}\hlnum{4}\hlstd{]}\hlopt{+}\hlkwd{rnorm}\hlstd{(n,}\hlnum{0}\hlstd{,}\hlkwc{sd}\hlstd{=}\hlnum{0.1}\hlstd{)}
  \hlstd{X} \hlkwb{<-} \hlkwd{cbind}\hlstd{(X1,X2,X3)}
  \hlstd{X} \hlkwb{=} \hlkwd{sweep}\hlstd{(X,} \hlnum{2}\hlstd{,} \hlkwd{apply}\hlstd{(X,}\hlnum{2}\hlstd{, mean))}  \hlcom{# subtract off the column means}
  \hlstd{Y} \hlkwb{=} \hlstd{betatrue[}\hlnum{1}\hlstd{]} \hlopt{+} \hlstd{X} \hlopt{%*%} \hlstd{betatrue[}\hlopt{-}\hlnum{1}\hlstd{]} \hlopt{+} \hlkwd{rnorm}\hlstd{(n,}\hlnum{0}\hlstd{,sigma)}  \hlcom{# does not have a column of ones for the intercept}
  \hlstd{df} \hlkwb{=} \hlkwd{data.frame}\hlstd{(Y, X)}
  \hlstd{fname[i]} \hlkwb{=} \hlkwd{paste}\hlstd{(}\hlstr{"df"}\hlstd{,} \hlkwd{as.character}\hlstd{(i),} \hlkwc{sep}\hlstd{=}\hlstr{""}\hlstd{)}
  \hlkwd{save}\hlstd{(df,} \hlkwc{file}\hlstd{=fname[i])}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}


Two of the
variables have a correlation of near 0.99, with the others more
modest.  Of the 20 variables, only 8 are related to $Y$.

\begin{enumerate}
  \item  Calculate the $\E[(\hat{\beta} - \beta)^T(\hat{\beta} - \beta)]$, the expected MSE for OLS under the full model.  (should be review!)
  \item   For each simulation, the OLS coefficients are found and an observed $\MSE = (\hat{\beta}^{(s)} - \beta)^T(\hat{\beta}^{(s)} -  \beta)$ is computed for each of the $s$ simulated datasets.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{MSE.OLS}  \hlkwb{=} \hlkwd{rep}\hlstd{(}\hlnum{NA}\hlstd{,nsim)}

\hlkwa{for}\hlstd{( i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{nsim) \{}
  \hlkwd{rm}\hlstd{(df)}
  \hlkwd{load}\hlstd{(fname[i])}
  \hlstd{nk.ols} \hlkwb{=} \hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{.,} \hlkwc{data}\hlstd{=df)}
  \hlstd{coef.ols} \hlkwb{=} \hlkwd{coef}\hlstd{(nk.ols)}
  \hlstd{MSE.OLS[i]} \hlkwb{=} \hlkwd{sum}\hlstd{((betatrue} \hlopt{-} \hlstd{coef.ols)}\hlopt{^}\hlnum{2}\hlstd{)}
\hlcom{#  print(c(i, MSE.OLS[i]))  for the truly bored student}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Does  the average of observed MSEs, 49.57, provide a good estimate  of the average of $E[(\hat{\beta} - \beta)^T(\hat{\beta} - \beta)]$? What does the distribution of observed MSE minus expected MSEs look like?   Do you think you should increase the number of simulated data sets?

\item For each of the data sets run lasso, ridge regression (lm.ridge from MASS or other), and the horseshoe (bhs from monomvn package on CRAN or other) to provide estimates of $\beta$  MSE, variance and squared bias for each simulated dataset; be careful about which methods standardize variables.







\item Derive the full conditional distributions for the Generalized double Pareto model of Armagan, Dunson, and Lee (2011)\footnote{Artin Armagan, David Dunson, \& Jaeyong Lee (2011). Generalized double Pareto shrinkage Statistica Sinica 23 (2013), 119-143 arXiv: 1104.0861v4 } (\url{https://arxiv.org/pdf/1104.0861v4.pdf})
(see Section 3) but with the Independent Jeffreys prior on $\beta_0$ and $\phi \equiv 1/\sigma^2$.

\item  Emplement the Generalized Double Pareto  and compare to the above methods, using the default $\alpha = \eta = 1$.
See
    Michael Lindon's implementation in R using Rcpp \url{https://michaellindon.github.io/lindonslog/mathematics/statistics/generalized-double-pareto-shrinkage-priors-sparse-estimation-regression/index.html}  or implement in JAGS.  Be careful about standardizing the predictors and change of variables for coefficients when computing the MSE!

\item In terms of MSE, which method(s) appears to be best (look  at average MSE and side-by-side boxplots)?  Which method has the least bias?  Which method has the most variability?

\item  (Optional)- repeat the above simulations, but now consider predictive MSE for  predicting new $\Y^*$'s at new $\X^*$ values with the same correlation  structure.  Are the methods that are best for estimating $\b$ also  best for estimating $\Y^*$?   Which seems to be harder?

\end{enumerate}

\item  For  the Gaussian ridge prior,  the lasso (Double exponential), independent Cauchy,  and the Generalized Double pareto, show whether the prior satisfy's any/all of the 3 conditions in Fan and Li (2011) \url{https://www.tandfonline.com/doi/abs/10.1198/016214501753382273}.

\end{enumerate}
\end{document}
